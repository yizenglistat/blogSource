{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "418ced5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# our softmax function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# word2vec, Skip-Gram, model\n",
    "class word2vec:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 2\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = 1\n",
    "        self.learning_rate = 0.001\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "  \n",
    "    def inital(self,V,data):\n",
    "        self.V = V\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.hidden_size, self.V))\n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (self.V, self.hidden_size))\n",
    "          \n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.center = np.dot(self.W1, x).reshape(self.hidden_size, 1)\n",
    "        self.output = np.dot(self.W2, self.center)\n",
    "        self.yhat = softmax(self.output)\n",
    "        return self.yhat\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # for W1\n",
    "        dJ_dW1 = np.dot(np.dot(self.W2.T, self.yhat - y), x.T)\n",
    "        # for W2\n",
    "        C = y.sum()\n",
    "        dJ_dW2 = np.dot(C*self.yhat - y, self.center.T)\n",
    "        # update\n",
    "        self.W1 = self.W1 - self.learning_rate * dJ_dW1\n",
    "        self.W2 = self.W2 - self.learning_rate * dJ_dW2\n",
    "\n",
    "    def train(self, max_epochs):\n",
    "        for epoch in range(1, max_epochs):       \n",
    "            self.loss = 0\n",
    "            for t in range(len(self.X_train)):\n",
    "                x = np.array(self.X_train)[t].reshape(self.V, 1)\n",
    "                y = np.array(self.y_train)[t].reshape(self.V, 1)\n",
    "                self.forward(x)\n",
    "                self.backward(x, y)\n",
    "                self.loss += -1*np.dot(y.T, np.log(self.yhat)).reshape(1,)[0]\n",
    "            print(f\"epoch={epoch} with loss={self.loss}\")\n",
    "            self.learning_rate *= 1/( (1+self.learning_rate*epoch) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3f70db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_data(corpus):\n",
    "    training_data = []\n",
    "    sentences = corpus.split(\".\")\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        sentence = sentences[i].split()\n",
    "        x = [word.lower() for word in sentence]\n",
    "        training_data.append(x)\n",
    "    return training_data\n",
    "\n",
    "def prepare_data(sentences, w2v):\n",
    "    \n",
    "    data = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in data:\n",
    "                data[word] = 1\n",
    "            else:\n",
    "                data[word] += 1\n",
    "    V = len(data)\n",
    "    data = sorted(list(data.keys()))\n",
    "    vocab = {}\n",
    "    for i in range(len(data)):\n",
    "        vocab[data[i]] = i\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            center_word = [0 for x in range(V)]\n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)]\n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size+1):\n",
    "                if i!=j and j>=0 and j<len(sentence):\n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word)\n",
    "            w2v.y_train.append(context)\n",
    "    \n",
    "    w2v.inital(V,data)\n",
    "  \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd3791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "270a9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"I want to buy an Apple iPhone. I want to eat an Apple now.\"\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "44e3e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 with loss=56.03171270146426\n",
      "epoch=2 with loss=56.00234199154801\n",
      "epoch=3 with loss=55.9731916998315\n",
      "epoch=4 with loss=55.94428780494471\n",
      "epoch=5 with loss=55.91565523681971\n",
      "epoch=6 with loss=55.88731774998322\n",
      "epoch=7 with loss=55.85929781094694\n",
      "epoch=8 with loss=55.831616500923374\n",
      "epoch=9 with loss=55.8042934347456\n"
     ]
    }
   ],
   "source": [
    "training_data = process_data(corpus)\n",
    "w2v = word2vec()\n",
    "_ = prepare_data(training_data, w2v)\n",
    "w2v.train(max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61ee5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd7409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567d8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
